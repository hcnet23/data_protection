\section{Introduction}
\label{sec:intro}

In 2012, a visual object classification system called AlexNet stood as the winner of the ImageNet challenge with a dominating performance \citep{AlexNet2012}. It achieved a top-5 error of 15.3\%, which is more than 10\% lower than that of the runner-up. Inspired by the monumental achievement, there has been impressive progress in machine learning research. Conventional machine learning approaches tend to show limitations in processing natural data since it typically involves careful feature engineering based on substantial domain expertise \citep{lecun2015deep}. Instead, a learning scheme, which enables the AlexNet, automates the overall feature extraction process. It automatically discovers salient representations of the raw data through a hierarchical learning process without any manual feature engineering. Due to the deep hierarchical structure of the learning scheme, it is often referred to as \textit{deep learning} (DL) or \textit{representation learning}.

The idea of building networks with more depth has revolutionized a variety of challenging tasks in machine learning, such as image generation \citep{rombach2021highresolution} and natural language processing (NLP) \citep{wei2022chain,wei2022finetuned}. Naturally, DL-based applications are rapidly becoming part of our everyday lives. For example, DL systems help to discover personalized items and prepare for rainy weather \citep{10.1145/3285029,Ravuri_2021}. Deep learning continues to surprise us with its endless possibilities. A DL system specifically taught to play a classical game Go, which is one of the most challenging games for artificial intelligence, defeats a Go world champion \citep{SilverHuangEtAl16nature}. This marked a major step in DL research since this achievement implies that deep learning can be adapted for any other purposes thanks to its general learning capability. 

The monumental achievements of DL systems seem to guarantee the absolute superiority and robustness of modern DL systems. However, DL systems have shown significant vulnerability to samples specifically crafted to misguide them, namely \textit{adversarial examples} \citep{Szegedy2014,goodfellow2015explaining,Dalvi04}. The adversarial examples are seemingly indistinguishable from original inputs, but they are imperceptibly perturbed to cause misbehavior of the systems. This confronts us with challenging questions regarding their analysis and interpretation. In response, various works attempt to elucidate the underlying causes of the brittleness. Notably, \citet{Ilyas2019} claimed that the brittleness is the result of the discrepancy between an objective we aim to achieve from the DL system and what we implicitly expect from the system throughout the optimization. Therefore, it is unreasonable to expect them to be robust to adversarial examples since the systems are not optimized to be resilient to them.

Arguably, fundamental works in adversarial robustness of deep learning systems have been mostly dominated by studies in computer vision \citep{Ilyas2019,zhang2019theoretically,Cohen19_RS}. The brittleness is, however, not unique to a specific domain. A number of prior studies have shown that deep NLP systems are also vulnerable to those manipulations at all stages of development and deployment \citep{Ebrahimi2018-hotflip,alzantot-etal-2018-generating,EmmaZhang2019,krishna2020thieves,tan-etal-2020-morphin,wallace-etal-2021-concealed,Wallace2019Triggers} (\cf \Cref{fig:adv_ex_text}). \citet{Ebrahimi2018-hotflip} showed that NLP systems can be fooled by a few perturbations of characters. \citet{zou-etal-2020-reinforced} successfully fooled machine translation systems to significantly lower its translation performance. \citet{Liu2023JailbreakingCV} presented a large-scale conversational agent \citep{chatgpt} can be misguided to generate malicious responses. \citet{wallace-etal-2021-concealed} even demonstrated that attackers can inject malicious examples into a training set of a victim model to corrupt the victim's behavior as the adversaries intended.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.8]{./Chapters/Introduction/figures/adv_examples.pdf}
	\caption{(a) A clean sample of IMDb \citep{imdb} and (b) its corresponding adversarial example. The adversarial example misguides a sentiment classification system to recognize the clean sample as an input text with negative sentiment by replacing `adaptation' and `missing' in the clean sample with `changing' and `evaporated', respectively.}
	\label{fig:adv_ex_text}
\end{figure}
%It is a necessity to safeguard the integrity and security of AI applications across domains. 

Building robust systems capable of withstanding adversarial attacks is not merely an academic pursuit. The brittleness could cause substantial problems as deep NLP systems have become ubiquitous and continued to move out of the lab into real-world applications, from chatbots \citep{chatgpt} to machine translations and text classifications. Despite the potential for sabotaging the phenomenal achievement, our conventions for developing deep NLP systems have not moved far enough toward robust NLP systems. The standard benchmarking approaches are still centered on training and evaluating them on clean samples without significant concerns about the dynamics and the variations of real-world problems. 

Overall, it is clear that we should aim beyond the accuracy and scalability of deep NLP systems to secure the things we can benefit through them. Equivalently, there is an urgent need for developing special approaches to address the brittleness of deep NLP systems, since the behaviors of the neural networks are not explicitly instructed in their weights and nodes as done in classical software engineering \citep{Hendrycks21_Safety}. Despite its importance, fundamental discussions about the adversarial robustness of NLP systems are still insignificant. We find that there still lacks a general consensus on the standard of benchmarking adversarial robustness in NLP, which makes it hard to have a unified view of existing work, such as the work on robustness of large pre-trained language models (PLMs) and attack efficacy of adversarial attack algorithms \citep{li-etal-2021-searching}. For instance, there exists a substantial gap between the robustness of the fine-tuned BERT \citep{bert} models across different studies \citep{Dong2021, yoo2021towardsimproving, zhang-etal-2022-improving,Hauser2021}. We also find that a simple adversarial example augmentation approach tends to outperform other sophisticated defense methods \citep{zhu2020freelb,wang2021infobert}. Moreover, there exists a challenge in bridging the gap between the theoretical findings from the image domain due to the discrete nature of texts. 

Consequently, experimental setups and defense strategies have been somewhat ad-hoc in adversarial NLP research field. This motivates us to delve into the problem in a rigorous manner from different technical perspectives and develop defense frameworks. Our research focus is specifically on the robustness of large-scale pre-trained language models (PLMs) since they have become the \textit{de-facto} choice for building NLP systems. Our studies across eight such PLMs and four tasks delve into an analysis of security holes of existing methods and probing the language representations of PLMs throughout fine-tuning, as well as transferability of adversarial samples across PLMs. We also analyze the threat of textual adversarial attacks from several technical perspectives. Our contributions and findings will serve as a solid guidance for future work in adversarial NLP.


For a seamless protection of NLP systems, we also propose two novel defense schemes working at various stages of NLP systems. Firstly, we propose a novel detection-based defense approach called GradMask. The proposed detection scheme identify adversarially perturbed tokens via a gradient signal. It provides several advantages over existing methods including improved detection performance and an interpretation of its decision with an only moderate computational cost. We also propose a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. The proposed defense scheme significantly outperforms existing defense schemes and demonstrate its effectiveness by extensive experiments. 


In the rest of this chapter, we will first discuss the key challenges and limitations of adversarial attacks in NLP in \Cref{sec:textual_attack} as adversarial attacks help to form a better understanding for building robust NLP systems. Subsequently, we present our novel defense schemes with a discussion about the limitations of existing approaches in \Cref{sec:defense_nlp}. Finally, we provide an outline of the dissertation in \Cref{sec:outlines}. 

